{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebdfe4eb-0494-4ede-9b7f-2e52ba4bf821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Yule-Walker Equations**\n",
    "\n",
    "The Yule-Walker can be used to find the optimal parameters for an AR(p) model.\n",
    "\n",
    "$$ x_t = \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + ... + \\phi_p x_{t-p} + w_t$$\n",
    "\n",
    "The equations arises as the solution of estimating the optimal coeffecients for Best Linear Prediction.\n",
    "Another way to derive the equations are to multiply by \\\\(x_{t-h}\\\\) and take the expectation of each sides of the AR(p) process.\n",
    "\n",
    "$$ E[x_t x_{t-h}] = \\phi_1 E[x_{t-1} x_{t-h}] + \\phi_2 E[x_{t-2} x_{t-h}] + ... + \\phi_p E[x_{t-p} x_{t-h}] + E[w_t x_{t-h}]$$\n",
    "\n",
    "Since \\\\(E[x_tx_{t-h}]=\\gamma(h)\\\\) and \\\\(w_t\\\\) does not depend on any prior \\\\(x_{t-h}\\\\) such that \\\\(E[w_t x_{t-h}]=0\\\\), we can simplify to.\n",
    " \n",
    "$$ \\gamma(h) = \\phi_1 \\gamma(h-1) + \\phi_2 \\gamma(h-2) + ... + \\phi_p \\gamma(h-p) $$\n",
    "\n",
    "This is known as the Yule-Walker recursion. For \\\\(h = 1,2,..,p\\\\) this makes up the Yule-Walker equations.\n",
    "\n",
    "The process noise variance can also be estimated using the Yule-Walker equations by considering the special case \\\\(h=0\\\\)\n",
    "\n",
    "$$ E[x_t x_t] = \\phi_1 E[x_{t-1} x_t] + \\phi_2 E[x_{t-2} x_t] + ... + \\phi_p E[x_{t-p} x_t] + E[w_t x_t]$$\n",
    "\n",
    "$$ \\gamma(0) = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(2) + ... + \\phi_p \\gamma(p) + E[w_t x_t]$$\n",
    "\n",
    "We can expand \\\\(E[w_t x_t]\\\\) to\n",
    "\n",
    "$$ E[w_t x_t] = E[w_t (\\phi_1 x_{t-1} + \\phi_2 x_{t-2} + ... + \\phi_p x_{t-p} + w_t)] = E[ \\phi_1 x_{t-1} w_t + \\phi_2 x_{t-2} w_t + ... + \\phi_p x_{t-p} w_t + w_t w_t)] = E[w_t w_t]=\\sigma_w^2$$\n",
    "\n",
    "Since again \\\\(w_t\\\\) does not depend on any prior \\\\(x_{t-h}\\\\)\n",
    "\n",
    "$$ \\gamma(0) = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(2) + ... + \\phi_p \\gamma(p) + \\sigma_w^2$$\n",
    "\n",
    "Solving for \\\\(\\sigma_w^2\\\\) yields\n",
    "\n",
    "$$ \\sigma_w^2 = \\gamma(0) - \\phi_1 \\gamma(1) + ... + \\phi_p \\gamma(p) $$\n",
    "\n",
    "All of this can be written in matrix notation as \n",
    "\n",
    "$$ \\mathbf{\\Gamma_p} \\mathbf{\\phi} = \\mathbf{\\gamma_p} $$\n",
    "\n",
    "$$ \\sigma_w^2 = \\gamma(0) - \\mathbf{\\gamma_p}^T\\mathbf{\\Gamma_p}^{-1} \\gamma_p = \\gamma(0) - \\mathbf{\\phi}^T \\mathbf{\\gamma}_p $$\n",
    "\n",
    "The Yule-Walker can be factored by \\\\(\\gamma(0)\\\\) to depend on the autocorrelation instead of the autocovariance.\n",
    "\n",
    "$$ \\widehat{\\mathbf{\\phi}} = \\widehat{\\mathbf{\\normalsize{R}}}_p^{-1} \\widehat{\\mathbf{\\rho}}_p $$\n",
    "\n",
    "$$ \\sigma_w^2 = \\widehat{\\gamma}(0)(1 - \\widehat{\\mathbf{\\rho}}_p^T \\widehat{\\mathbf{\\normalsize{R}}}_p^{-1} \\widehat{\\rho}_p) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36872046-e6ab-4ee7-a4ec-4b312e1a17f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define autocovariance\n",
    "def autocovariance(x, lag):\n",
    "    n = len(x)\n",
    "    mean_x = np.mean(x)\n",
    "    return np.sum((x[:n - lag] - mean_x) * (x[lag:] - mean_x)) / n\n",
    "\n",
    "## Define autocovariance matrix\n",
    "def autocov_matrix(x, max_lag):\n",
    "    return np.array([[autocovariance(x, abs(i - j)) for j in range(max_lag + 1)] for i in range(max_lag + 1)])\n",
    "\n",
    "def gen_ar_data(num_samples, sigma, mu, phi):\n",
    "    w = sigma * np.random.randn(num_samples) + mu\n",
    "    x = np.zeros(num_samples)   \n",
    "    \n",
    "    # Generate data for the model\n",
    "    # x[n] = phi_1 * x[n-1] + phi_2 * x[n-2] + .. + phi_k * x[n-k] + w[n]\n",
    "    x = signal.lfilter(b=[1], a=[1] + [-phi_ for phi_ in phi], x=w)\n",
    "    return x, w\n",
    "\n",
    "def calc_ar_coefs(x, num_model_coefs):\n",
    "    cov = autocov_matrix(x, num_model_coefs)\n",
    "    # Solve the equation Ax=b to find the coefficients\n",
    "    Gamma = cov[-num_model_coefs:, -num_model_coefs:]\n",
    "    gamma = cov[1:, 0]\n",
    "    coefs = np.linalg.inv(Gamma) @ gamma\n",
    "    return coefs, cov, Gamma, gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afafd4fd-72c2-4550-b22c-77e0b0842b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example on generating AR(2) process data and estimating the parameters\n",
    "\n",
    "N = 1000\n",
    "mu, sigma = 0, 0.1\n",
    "phi = [1.5, -0.75]\n",
    "# We know the number of model coeficients\n",
    "num_model_coefs = len(phi)\n",
    "\n",
    "x, w = gen_ar_data(N, sigma, mu, phi)\n",
    "\n",
    "coefs, cov, Gamma, gamma = calc_ar_coefs(x, num_model_coefs)\n",
    "\n",
    "print(f\"Phi: {phi}, estimated phi: {coefs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c6dfbff-9c70-43ee-813b-4b440e35130d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Autocovariance matrix of an AR(2) process**\n",
    "\n",
    "Using the Yule-Walker equations and an AR(2) process, we can find the autocovariance matrix.\n",
    "\n",
    "The Yule-Walker equations for an AR(2) process are\n",
    "\n",
    "$$ \\gamma(0) = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(2) + \\sigma_w^2 $$\n",
    "\n",
    "$$ \\gamma(1) = \\phi_1 \\gamma(0) + \\phi_2 \\gamma(1) $$\n",
    "\n",
    "$$ \\gamma(2) = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(0) $$\n",
    "\n",
    "This can be rewritten to\n",
    "\n",
    "$$ \\gamma(0) - \\phi_1 \\gamma(1) - \\phi_2 \\gamma(2) = \\sigma_w^2 $$\n",
    "\n",
    "$$ - \\phi_1 \\gamma(0) + \\gamma(1) (1-\\phi_2) = 0 $$\n",
    "\n",
    "$$ - \\phi_2 \\gamma(0) - \\phi_1 \\gamma(1) + \\gamma(2) = 0 $$\n",
    "\n",
    "In matrix notation this becomes\n",
    "\n",
    "$$ \\mathbf{\\Phi} \\mathbf{\\gamma} = [\\sigma^2, 0, 0]^T$$\n",
    "\n",
    "Where \\\\(\\mathbf{\\gamma}=[\\gamma(0), \\gamma(1), \\gamma(2)]^T\\\\) and \\\\(\\mathbf{\\Phi} =[[1, -\\phi_1, -phi_2],[-phi_1, (1-phi_2), 0],[-phi_2, -phi_1, 1]]\\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa281605-aeb6-4db5-8c74-182801a5ae8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Example on how to calculate the covariance matrix given an AR(2) process\n",
    "# It can be seen that as N increases, the esimated covariance matrix converges to the true covariance matrix\n",
    "\n",
    "Ns = [2**i for i in range(6, 16)]\n",
    "num_tests = 100\n",
    "\n",
    "# Calulate true covariance matrix\n",
    "Phi = np.array([[1, -phi[0], -phi[1]],\n",
    "                [-phi[0], (1-phi[1]), 0],\n",
    "                [-phi[1], -phi[0], 1]])\n",
    "\n",
    "b = [sigma**2, 0, 0]\n",
    "\n",
    "gamma_true = np.linalg.inv(Phi) @ b\n",
    "Gamma_true = np.array([[gamma_true[0], gamma_true[1], gamma_true[2]],\n",
    "                       [gamma_true[1], gamma_true[0], gamma_true[1]],\n",
    "                       [gamma_true[2], gamma_true[1], gamma_true[0]]])\n",
    "\n",
    "covariance_errors = np.zeros(len(Ns))\n",
    "\n",
    "for n, N in enumerate(Ns):\n",
    "    for i in range(num_tests):\n",
    "        x, w = gen_ar_data(N, sigma, mu, phi)\n",
    "\n",
    "        coefs, cov, Gamma, gamma = calc_ar_coefs(x, num_model_coefs)\n",
    "        error = np.sum(np.abs(cov - Gamma_true))\n",
    "        covariance_errors[n] += error\n",
    "\n",
    "covariance_errors = covariance_errors / num_tests\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Ns, covariance_errors, label=\"Covariance matrix error\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff39a47-16eb-48f9-aa33-cbe68edffec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Property - Large Sample Error of Yule-Walker Estimators**\n",
    "\n",
    "The asymptotic behaviour as \\\\(N \\rightarrow \\infin\\\\) of the Yule-Walker Estimators in an causal AR(p) is as following\n",
    "\n",
    "$$ \\sqrt(n) (\\mathbf{\\widehat{\\phi}} - \\mathbf{\\phi}) \\xrightarrow{d} \\mathcal{N}(\\mu, \\sigma_w^2 \\Gamma_p^{-1})$$\n",
    "\n",
    "Here \\\\(\\Gamma_p\\\\) is the autocovariance toeplitx matrix with values from \\\\(\\gamma(0)\\\\) to \\\\(\\gamma(p)\\\\).\n",
    "\n",
    "As N gets larger, the error will converge in distribution towards being normally distributed with \\\\(\\mu=0\\\\) and \\\\(\\sigma^2 = \\sigma_w^2 \\Gamma_p^{-1}\\\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e725929a-5c8c-4a74-b24c-4176e7cdda96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In this example it is see n how the error distribution of parameters\n",
    "# converges towards the theoretical distribution \n",
    "\n",
    "num_samples_test = [2**i for i in range(10, 20)]\n",
    "num_test = 100\n",
    "\n",
    "estimator_errors = np.zeros((len(num_samples_test), num_test))\n",
    "estimated_noise_variance = np.zeros((len(num_samples_test), num_test))\n",
    "estimator_errors_var_theoretical = np.zeros(len(num_samples_test))\n",
    "\n",
    "\n",
    "for i, num_samples in enumerate(num_samples_test):\n",
    "    for j in range(num_test):\n",
    "\n",
    "        x, w = gen_ar_data(num_samples, sigma, mu, phi)\n",
    "\n",
    "        coefs, cov, Gamma, gamma = calc_ar_coefs(x, num_model_coefs)\n",
    "        estimator_errors[i, j] = (coefs[0] - phi[0])\n",
    "        estimated_noise_variance[i, j] = cov[0, 0] - gamma.T @ np.linalg.inv(Gamma) @ gamma\n",
    "\n",
    "    phi_cov = (sigma**2)/num_samples * np.linalg.inv(Gamma_true[:2, :2])\n",
    "    \n",
    "    estimator_errors_var_theoretical[i] = phi_cov[0, 0]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_samples_test, estimator_errors_var_theoretical, label=\"Estimator error variance theoretical\")\n",
    "plt.plot(num_samples_test, np.var(estimator_errors, axis=1, ddof=1), label=\"Estimator error variance empirical\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_samples_test, np.zeros_like(num_samples_test), label=\"Estimator error mean theoretical\")\n",
    "plt.plot(num_samples_test, np.mean(estimator_errors, axis=1), label=\"Estimator error mean empirical\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_samples_test, np.mean(estimated_noise_variance, axis=1), label=\"Estimated noise variance\")\n",
    "plt.plot(num_samples_test, np.ones(len(num_samples_test))*sigma**2, label=\"True noise variance\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Noise variance\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d062a57-b1a2-4d87-8470-e1a962e9d54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Maximum likelihood estimation of parameters**\n",
    "\n",
    "Another way of estimating AR parameters is using a maximum likelihood method.\n",
    "\n",
    "Consider the AR(1) process\n",
    "\n",
    "$$ x_t = \\phi x_{t-1} + w_t $$\n",
    "\n",
    "Where \\\\(w_t \\sim \\mathcal{N}(0, \\sigma_w^2)\\\\) and \\\\(x_t \\sim \\mathcal{N}(0, \\frac{\\sigma_w^2}{1-\\phi^2})\\\\)\n",
    "\n",
    "The joint likelihood, for \\\\(x_1, x_2,..,x_T\\\\) is\n",
    "\n",
    "$$ L(\\phi, \\sigma_w^2) = f(x_1, x_2,..,x_T |\\phi, \\sigma_w^2) $$\n",
    "\n",
    "The likelihood is the measure of how likely we are to observe \\\\([x_1, x_2,..,x_T]\\\\) given the model parameters \\\\([\\phi, \\sigma_w^2]\\\\)\n",
    "\n",
    "Since \\\\(x_1\\\\) does not depend on any future values of \\\\(x_t\\\\) we can factor it out.\n",
    "For convenience, the model parameters are left out.\n",
    "\n",
    "$$ f(x_1, x_2,..,x_T) = f(x_1)f(x_2,..,x_T) $$\n",
    "\n",
    "Using the chain rule of probability \\\\(P(A,B) = P(A|B)P(B)\\\\) we can split up the likelihood function. We can factor out \\\\(f(x_2)\\\\) if we condition on already knowing \\\\(x_1\\\\). \n",
    "\n",
    "$$ f(x_1, x_2,..,x_T) = f(x_1)f(x_2|x_1)f(x_3,..,x_T) $$\n",
    "\n",
    "We can continue to do this indefinitely which will give us\n",
    "\n",
    "$$ f(x_1, x_2,..,x_T) = f(x_1) \\prod_{t=2}^T f(x_k|x_{t-1}) $$\n",
    "\n",
    "We consider \\\\(x_1\\\\) is known, so \\\\(f(x_1)=1\\\\) and the likelihood function becomes\n",
    "\n",
    "$$ L(\\phi, \\sigma_w^2) = \\prod_{t=2}^T f(x_k|x_{t-1};\\phi, \\sigma_w^2) $$\n",
    "\n",
    "Here \\\\(x_t|x_{t-1} \\sim \\mathcal{N}(\\phi x_{t-1}, \\sigma_w^2)\\\\) since \\\\(x_t\\\\) is made up of the known quantity \\\\(\\phi x_{t-1}\\\\) and the random variable \\\\(w_t\\\\)\n",
    "\n",
    "$$ L(\\phi, \\sigma_w^2) = \\prod_{t=2}^T \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x_t-\\phi x_{t-1})^2}{2 \\sigma_w^2}} $$\n",
    "\n",
    "We find the log likelihood by taking the logarithm to both sides\n",
    "\n",
    "$$ \\ell(\\phi, \\sigma_w^2) = - \\frac{T-1}{2}log(2 \\pi \\sigma_w^2) - \\frac{1}{2 \\pi \\sigma_w^2} \\sum_{t=2}^T(x_t - \\phi x_{t-1})^2 $$\n",
    "\n",
    "We want to maximize the likelihood and hence maximize the log likelihood. This is done with respect to \\\\( \\phi \\\\). To maximize the log likelihood, we minimize the expression \\\\(S(\\phi)\\sum_{t=2}^T(x_t - \\phi x_{t-1})^2\\\\)\n",
    "\n",
    "This can either be done by taking the derivative with respect to \\\\(\\phi\\\\), setting it equal to 0 and solving. However, we recognize the term as the sum of least squares, which means that when we condition on one step ahead, the MLE is also the least squates estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3236a5e7-193b-4fe0-bbc3-85bc87a9d7df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example showing how to estimate the AR process parameters using the MLE\n",
    "\n",
    "N = 50\n",
    "mu, sigma = 0, 0.1\n",
    "phi = 0.5\n",
    "\n",
    "w = sigma * np.random.randn(N) + mu\n",
    "x = np.zeros(N)\n",
    "x_hat = np.zeros(N)   \n",
    "\n",
    "x[0] = w[0]\n",
    "for i in range(1, len(x)):\n",
    "    x[i] = phi * x[i-1] + w[i]\n",
    "\n",
    "# Construct y\n",
    "y = x[1:]\n",
    "v = x[:-1]\n",
    "\n",
    "# Solve the OLS linear regression problem\n",
    "phi_est =  np.dot(v, y) / (np.dot(v,v))\n",
    "\n",
    "print(f\"Coefficient estimated with MLE: {phi_est}\")\n",
    "\n",
    "x_hat[0] = w[0]\n",
    "for i in range(1, len(x_hat)):\n",
    "    x_hat[i] = phi_est * x_hat[i-1] + w[i]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, label=\"True signal\")\n",
    "plt.plot(x_hat, label=\"Estimated signal\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "plt.title(\"Estimated signal vs true signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045bcb3b-4f2f-4e14-88dc-da812804d601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "** Estimation of both AR and MA parameters **\n",
    "\n",
    "Given an ARMA(p,q) model\n",
    "\n",
    "$$ x_t = \\sum_{i=1}^p \\phi_i x_{t-i} + w_t + \\sum_{j=1}^q \\theta_j w_{t-j} $$\n",
    "\n",
    "We see that for estimating the AR paramters \\\\(\\phi\\\\) we can apply linear regression to find the paramters \\\\(\\phi\\\\) that give the best linear combination of \\\\(x_{t-1},x_{t-2},..,x_{t-p}\\\\) that equals \\\\(x_t\\\\).\n",
    "\n",
    "However, for the MA part of the process, we cannot directly observe the \\\\(w_t\\\\) terms and since they are dependent on the model paramters. Hence, to estimate the model parameters, we need to estimate the errors, which in turn depends in the model parameters. This is a non-linear problem without any closed form solution and we'll need numerical methods to solve it.\n",
    "\n",
    "**Guass-Newton method**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ARMA Parameter Estimation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
