{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebdfe4eb-0494-4ede-9b7f-2e52ba4bf821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Yule-Walker Equations**\n",
    "\n",
    "The Yule-Walker can be used to find the optimal parameters for an AR(p) model.\n",
    "\n",
    "$$ x_t = \\phi_1 x_{t-1} + \\phi_2 x_{t-2} + ... + \\phi_p x_{t-p} + w_t$$\n",
    "\n",
    "The equations arises as the solution of estimating the optimal coeffecients for Best Linear Prediction.\n",
    "Another way to derive the equations are to multiply by \\\\(x_{t-h}\\\\) and take the expectation of each sides of the AR(p) process.\n",
    "\n",
    "$$ E[x_t x_{t-h}] = \\phi_1 E[x_{t-1} x_{t-h}] + \\phi_2 E[x_{t-2} x_{t-h}] + ... + \\phi_p E[x_{t-p} x_{t-h}] + E[w_t x_{t-h}]$$\n",
    "\n",
    "Since \\\\(E[x_tx_{t-h}]=\\gamma(h)\\\\) and \\\\(w_t\\\\) does not depend on any prior \\\\(x_{t-h}\\\\) such that \\\\(E[w_t x_{t-h}]=0\\\\), we can simplify to.\n",
    " \n",
    "$$ \\gamma(h) = \\phi_1 \\gamma(h-1) + \\phi_2 \\gamma(h-2) + ... + \\phi_p \\gamma(h-p) $$\n",
    "\n",
    "This is known as the Yule-Walker recursion. For \\\\(h = 1,2,..,p\\\\) this makes up the Yule-Walker equations.\n",
    "\n",
    "The process noise variance can also be estimated using the Yule-Walker equations by considering the special case \\\\(h=0\\\\)\n",
    "\n",
    "$$ E[x_t x_t] = \\phi_1 E[x_{t-1} x_t] + \\phi_2 E[x_{t-2} x_t] + ... + \\phi_p E[x_{t-p} x_t] + E[w_t x_t]$$\n",
    "\n",
    "$$ \\gamma(0) = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(2) + ... + \\phi_p \\gamma(p) + E[w_t x_t]$$\n",
    "\n",
    "We can expand \\\\(E[w_t x_t]\\\\) to\n",
    "\n",
    "$$ E[w_t x_t] = E[w_t (\\phi_1 x_{t-1} + \\phi_2 x_{t-2} + ... + \\phi_p x_{t-p} + w_t)] = E[ \\phi_1 x_{t-1} w_t + \\phi_2 x_{t-2} w_t + ... + \\phi_p x_{t-p} w_t + w_t w_t)] = E[w_t w_t]=\\sigma_w^2$$\n",
    "\n",
    "Since again \\\\(w_t\\\\) does not depend on any prior \\\\(x_{t-h}\\\\)\n",
    "\n",
    "$$ \\gamma(0) = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(2) + ... + \\phi_p \\gamma(p) + \\sigma_w^2$$\n",
    "\n",
    "Solving for \\\\(\\sigma_w^2\\\\) yields\n",
    "\n",
    "$$ \\sigma_w^2 = \\gamma(0) - \\phi_1 \\gamma(1) + ... + \\phi_p \\gamma(p) $$\n",
    "\n",
    "All of this can be written in matrix notation as \n",
    "\n",
    "$$ \\mathbf{\\Gamma_p} \\mathbf{\\phi} = \\mathbf{\\gamma_p} $$\n",
    "\n",
    "$$ \\sigma_w^2 = \\gamma(0) - \\mathbf{\\gamma_p}^T\\mathbf{\\Gamma_p}^{-1} \\gamma_p = \\gamma(0) - \\mathbf{\\phi}^T \\mathbf{\\gamma}_p $$\n",
    "\n",
    "The Yule-Walker can be factored by \\\\(\\gamma(0)\\\\) to depend on the autocorrelation instead of the autocovariance.\n",
    "\n",
    "$$ \\widehat{\\mathbf{\\phi}} = \\widehat{\\mathbf{\\normalsize{R}}}_p^{-1} \\widehat{\\mathbf{\\rho}}_p $$\n",
    "\n",
    "$$ \\sigma_w^2 = \\widehat{\\gamma}(0)(1 - \\widehat{\\mathbf{\\rho}}_p^T \\widehat{\\mathbf{\\normalsize{R}}}_p^{-1} \\widehat{\\rho}_p) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36872046-e6ab-4ee7-a4ec-4b312e1a17f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define autocovariance\n",
    "def autocovariance(x, lag):\n",
    "    n = len(x)\n",
    "    mean_x = np.mean(x)\n",
    "    return np.sum((x[:n - lag] - mean_x) * (x[lag:] - mean_x)) / n\n",
    "\n",
    "## Define autocovariance matrix\n",
    "def autocov_matrix(x, max_lag):\n",
    "    return np.array([[autocovariance(x, abs(i - j)) for j in range(max_lag + 1)] for i in range(max_lag + 1)])\n",
    "\n",
    "def gen_ar_data(num_samples, sigma, mu, phi):\n",
    "    w = sigma * np.random.randn(num_samples) + mu\n",
    "    x = np.zeros(num_samples)   \n",
    "    \n",
    "    # Generate data for the model\n",
    "    # x[n] = phi_1 * x[n-1] + phi_2 * x[n-2] + .. + phi_k * x[n-k] + w[n]\n",
    "    x = signal.lfilter(b=[1], a=[1] + [-phi_ for phi_ in phi], x=w)\n",
    "    return x, w\n",
    "\n",
    "def calc_ar_coefs(x, num_model_coefs):\n",
    "    cov = autocov_matrix(x, num_model_coefs)\n",
    "    # Solve the equation Ax=b to find the coefficients\n",
    "    Gamma = cov[-num_model_coefs:, -num_model_coefs:]\n",
    "    gamma = cov[1:, 0]\n",
    "    coefs = np.linalg.inv(Gamma) @ gamma\n",
    "    return coefs, cov, Gamma, gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afafd4fd-72c2-4550-b22c-77e0b0842b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example on generating AR(2) process data and estimating the parameters\n",
    "\n",
    "N = 1000\n",
    "mu, sigma = 0, 0.1\n",
    "phi = [1.5, -0.75]\n",
    "# We know the number of model coeficients\n",
    "num_model_coefs = len(phi)\n",
    "\n",
    "x, w = gen_ar_data(N, sigma, mu, phi)\n",
    "\n",
    "coefs, cov, Gamma, gamma = calc_ar_coefs(x, num_model_coefs)\n",
    "\n",
    "print(f\"Phi: {phi}, estimated phi: {coefs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c6dfbff-9c70-43ee-813b-4b440e35130d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Autocovariance matrix of an AR(2) process**\n",
    "\n",
    "Using the Yule-Walker equations and an AR(2) process, we can find the autocovariance matrix.\n",
    "\n",
    "The Yule-Walker equations for an AR(2) process are\n",
    "\n",
    "$$ \\gamma(0) = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(2) + \\sigma_w^2 $$\n",
    "\n",
    "$$ \\gamma(1) = \\phi_1 \\gamma(0) + \\phi_2 \\gamma(1) $$\n",
    "\n",
    "$$ \\gamma(2) = \\phi_1 \\gamma(1) + \\phi_2 \\gamma(0) $$\n",
    "\n",
    "This can be rewritten to\n",
    "\n",
    "$$ \\gamma(0) - \\phi_1 \\gamma(1) - \\phi_2 \\gamma(2) = \\sigma_w^2 $$\n",
    "\n",
    "$$ - \\phi_1 \\gamma(0) + \\gamma(1) (1-\\phi_2) = 0 $$\n",
    "\n",
    "$$ - \\phi_2 \\gamma(0) - \\phi_1 \\gamma(1) + \\gamma(2) = 0 $$\n",
    "\n",
    "In matrix notation this becomes\n",
    "\n",
    "$$ \\mathbf{\\Phi} \\mathbf{\\gamma} = [\\sigma^2, 0, 0]^T$$\n",
    "\n",
    "Where \\\\(\\mathbf{\\gamma}=[\\gamma(0), \\gamma(1), \\gamma(2)]^T\\\\) and \\\\(\\mathbf{\\Phi} =[[1, -\\phi_1, -phi_2],[-phi_1, (1-phi_2), 0],[-phi_2, -phi_1, 1]]\\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa281605-aeb6-4db5-8c74-182801a5ae8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Example on how to calculate the covariance matrix given an AR(2) process\n",
    "# It can be seen that as N increases, the esimated covariance matrix converges to the true covariance matrix\n",
    "\n",
    "Ns = [2**i for i in range(6, 16)]\n",
    "num_tests = 100\n",
    "\n",
    "# Calulate true covariance matrix\n",
    "Phi = np.array([[1, -phi[0], -phi[1]],\n",
    "                [-phi[0], (1-phi[1]), 0],\n",
    "                [-phi[1], -phi[0], 1]])\n",
    "\n",
    "b = [sigma**2, 0, 0]\n",
    "\n",
    "gamma_true = np.linalg.inv(Phi) @ b\n",
    "Gamma_true = np.array([[gamma_true[0], gamma_true[1], gamma_true[2]],\n",
    "                       [gamma_true[1], gamma_true[0], gamma_true[1]],\n",
    "                       [gamma_true[2], gamma_true[1], gamma_true[0]]])\n",
    "\n",
    "covariance_errors = np.zeros(len(Ns))\n",
    "\n",
    "for n, N in enumerate(Ns):\n",
    "    for i in range(num_tests):\n",
    "        x, w = gen_ar_data(N, sigma, mu, phi)\n",
    "\n",
    "        coefs, cov, Gamma, gamma = calc_ar_coefs(x, num_model_coefs)\n",
    "        error = np.sum(np.abs(cov - Gamma_true))\n",
    "        covariance_errors[n] += error\n",
    "\n",
    "covariance_errors = covariance_errors / num_tests\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Ns, covariance_errors, label=\"Covariance matrix error\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff39a47-16eb-48f9-aa33-cbe68edffec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Property - Large Sample Error of Yule-Walker Estimators**\n",
    "\n",
    "The asymptotic behaviour as \\\\(N \\rightarrow \\infin\\\\) of the Yule-Walker Estimators in an causal AR(p) is as following\n",
    "\n",
    "$$ \\sqrt(n) (\\mathbf{\\widehat{\\phi}} - \\mathbf{\\phi}) \\xrightarrow{d} \\mathcal{N}(\\mu, \\sigma_w^2 \\Gamma_p^{-1})$$\n",
    "\n",
    "Here \\\\(\\Gamma_p\\\\) is the autocovariance toeplitx matrix with values from \\\\(\\gamma(0)\\\\) to \\\\(\\gamma(p)\\\\).\n",
    "\n",
    "As N gets larger, the error will converge in distribution towards being normally distributed with \\\\(\\mu=0\\\\) and \\\\(\\sigma^2 = \\sigma_w^2 \\Gamma_p^{-1}\\\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e725929a-5c8c-4a74-b24c-4176e7cdda96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In this example it is see n how the error distribution of parameters\n",
    "# converges towards the theoretical distribution \n",
    "\n",
    "num_samples_test = [2**i for i in range(10, 20)]\n",
    "num_test = 100\n",
    "\n",
    "estimator_errors = np.zeros((len(num_samples_test), num_test))\n",
    "estimated_noise_variance = np.zeros((len(num_samples_test), num_test))\n",
    "estimator_errors_var_theoretical = np.zeros(len(num_samples_test))\n",
    "\n",
    "\n",
    "for i, num_samples in enumerate(num_samples_test):\n",
    "    for j in range(num_test):\n",
    "\n",
    "        x, w = gen_ar_data(num_samples, sigma, mu, phi)\n",
    "\n",
    "        coefs, cov, Gamma, gamma = calc_ar_coefs(x, num_model_coefs)\n",
    "        estimator_errors[i, j] = (coefs[0] - phi[0])\n",
    "        estimated_noise_variance[i, j] = cov[0, 0] - gamma.T @ np.linalg.inv(Gamma) @ gamma\n",
    "\n",
    "    phi_cov = (sigma**2)/num_samples * np.linalg.inv(Gamma_true[:2, :2])\n",
    "    \n",
    "    estimator_errors_var_theoretical[i] = phi_cov[0, 0]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_samples_test, estimator_errors_var_theoretical, label=\"Estimator error variance theoretical\")\n",
    "plt.plot(num_samples_test, np.var(estimator_errors, axis=1, ddof=1), label=\"Estimator error variance empirical\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_samples_test, np.zeros_like(num_samples_test), label=\"Estimator error mean theoretical\")\n",
    "plt.plot(num_samples_test, np.mean(estimator_errors, axis=1), label=\"Estimator error mean empirical\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_samples_test, np.mean(estimated_noise_variance, axis=1), label=\"Estimated noise variance\")\n",
    "plt.plot(num_samples_test, np.ones(len(num_samples_test))*sigma**2, label=\"True noise variance\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Noise variance\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d062a57-b1a2-4d87-8470-e1a962e9d54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Maximum likelihood estimation of parameters**\n",
    "\n",
    "Another way of estimating AR parameters is using a maximum likelihood method.\n",
    "\n",
    "Consider the AR(1) process\n",
    "\n",
    "$$ x_t = \\phi x_{t-1} + w_t $$\n",
    "\n",
    "Where \\\\(w_t \\sim \\mathcal{N}(0, \\sigma_w^2)\\\\) and \\\\(x_t \\sim \\mathcal{N}(0, \\frac{\\sigma_w^2}{1-\\phi^2})\\\\)\n",
    "\n",
    "The joint likelihood, for \\\\(x_1, x_2,..,x_T\\\\) is\n",
    "\n",
    "$$ L(\\phi, \\sigma_w^2) = f(x_1, x_2,..,x_T |\\phi, \\sigma_w^2) $$\n",
    "\n",
    "The likelihood is the measure of how likely we are to observe \\\\([x_1, x_2,..,x_T]\\\\) given the model parameters \\\\([\\phi, \\sigma_w^2]\\\\)\n",
    "\n",
    "Since \\\\(x_1\\\\) does not depend on any future values of \\\\(x_t\\\\) we can factor it out.\n",
    "For convenience, the model parameters are left out.\n",
    "\n",
    "$$ f(x_1, x_2,..,x_T) = f(x_1)f(x_2,..,x_T) $$\n",
    "\n",
    "Using the chain rule of probability \\\\(P(A,B) = P(A|B)P(B)\\\\) we can split up the likelihood function. We can factor out \\\\(f(x_2)\\\\) if we condition on already knowing \\\\(x_1\\\\). \n",
    "\n",
    "$$ f(x_1, x_2,..,x_T) = f(x_1)f(x_2|x_1)f(x_3,..,x_T) $$\n",
    "\n",
    "We can continue to do this indefinitely which will give us\n",
    "\n",
    "$$ f(x_1, x_2,..,x_T) = f(x_1) \\prod_{t=2}^T f(x_k|x_{t-1}) $$\n",
    "\n",
    "We consider \\\\(x_1\\\\) is known, so \\\\(f(x_1)=1\\\\) and the likelihood function becomes\n",
    "\n",
    "$$ L(\\phi, \\sigma_w^2) = \\prod_{t=2}^T f(x_k|x_{t-1};\\phi, \\sigma_w^2) $$\n",
    "\n",
    "Here \\\\(x_t|x_{t-1} \\sim \\mathcal{N}(\\phi x_{t-1}, \\sigma_w^2)\\\\) since \\\\(x_t\\\\) is made up of the known quantity \\\\(\\phi x_{t-1}\\\\) and the random variable \\\\(w_t\\\\)\n",
    "\n",
    "$$ L(\\phi, \\sigma_w^2) = \\prod_{t=2}^T \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x_t-\\phi x_{t-1})^2}{2 \\sigma_w^2}} $$\n",
    "\n",
    "We find the log likelihood by taking the logarithm to both sides\n",
    "\n",
    "$$ \\ell(\\phi, \\sigma_w^2) = - \\frac{T-1}{2}log(2 \\pi \\sigma_w^2) - \\frac{1}{2 \\pi \\sigma_w^2} \\sum_{t=2}^T(x_t - \\phi x_{t-1})^2 $$\n",
    "\n",
    "We want to maximize the likelihood and hence maximize the log likelihood. This is done with respect to \\\\( \\phi \\\\). To maximize the log likelihood, we minimize the expression \\\\(S(\\phi)\\sum_{t=2}^T(x_t - \\phi x_{t-1})^2\\\\)\n",
    "\n",
    "This can either be done by taking the derivative with respect to \\\\(\\phi\\\\), setting it equal to 0 and solving. However, we recognize the term as the sum of least squares, which means that when we condition on one step ahead, the MLE is also the least squates estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3236a5e7-193b-4fe0-bbc3-85bc87a9d7df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example showing how to estimate the AR process parameters using the MLE\n",
    "\n",
    "N = 50\n",
    "mu, sigma = 0, 0.1\n",
    "phi = 0.5\n",
    "\n",
    "w = sigma * np.random.randn(N) + mu\n",
    "x = np.zeros(N)\n",
    "x_hat = np.zeros(N)   \n",
    "\n",
    "x[0] = w[0]\n",
    "for i in range(1, len(x)):\n",
    "    x[i] = phi * x[i-1] + w[i]\n",
    "\n",
    "def yule_walker_estimate_ar_params(x):\n",
    "    # Construct y\n",
    "    y = x[1:]\n",
    "    v = x[:-1]\n",
    "\n",
    "    # Solve the OLS linear regression problem\n",
    "    phi_est =  np.dot(v, y) / (np.dot(v,v))\n",
    "    return phi_est\n",
    "\n",
    "phi_est = yule_walker_estimate_ar_params(x)\n",
    "\n",
    "print(f\"Coefficient estimated with MLE: {phi_est}\")\n",
    "\n",
    "x_hat[0] = w[0]\n",
    "for i in range(1, len(x_hat)):\n",
    "    x_hat[i] = phi_est * x_hat[i-1] + w[i]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, label=\"True signal\")\n",
    "plt.plot(x_hat, label=\"Estimated signal\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "plt.title(\"Estimated signal vs true signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045bcb3b-4f2f-4e14-88dc-da812804d601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "** Estimation of both AR and MA parameters **\n",
    "\n",
    "Given an ARMA(p,q) model\n",
    "\n",
    "$$ x_t = \\sum_{i=1}^p \\phi_i x_{t-i} + w_t + \\sum_{j=1}^q \\theta_j w_{t-j} $$\n",
    "\n",
    "We see that for estimating the AR paramters \\\\(\\phi\\\\) we can apply linear regression to find the paramters \\\\(\\phi\\\\) that give the best linear combination of \\\\(x_{t-1},x_{t-2},..,x_{t-p}\\\\) that equals \\\\(x_t\\\\).\n",
    "\n",
    "Isolating the current error \\\\(w_t\\\\) and letting it depend on the model parameters gives us\n",
    "\n",
    "$$ w_t(\\mathbf{\\beta}) = x_t - \\sum_{i=1}^p \\phi_i x_{t-i} - \\sum_{j=1}^q \\theta_j w_{t-j}(\\mathbf{\\beta}) $$\n",
    "\n",
    "Here we defined \\\\(\\mathbf{\\beta}=[\\phi_1,\\phi_2,..,\\phi_p,\\theta_1,\\theta_1,..,\\theta_1,]\\\\)\n",
    "\n",
    "We want to find \\\\(\\mathbf{\\beta}\\\\) such that the sum of squares \\\\(S(\\mathbf{\\beta})\\\\) is minimized.\n",
    "\n",
    "$$ S(\\mathbf{\\beta}) = \\sum_{t=r+1}^{T}w_t^2(\\mathbf{\\beta}) $$\n",
    "\n",
    "Where \\\\(r=max(p,q)\\\\)\n",
    "\n",
    "If \\\\(q=0\\\\) our problem becomes least squares as shown the previous examples, but if \\\\(q>0\\\\) we will have error terms that depend on \\\\(\\mathbf{\\beta}\\\\). Hence, to estimate the model parameters, we need to estimate the errors, which in turn depends in the model parameters. \n",
    "Since the error is a function of \\\\(\\mathbf{\\beta}\\\\) and it is taken to the second power, it is a non-linear problem without any closed form solution and we'll need numerical methods to solve it.\n",
    "\n",
    "**Gauss-Newton method**\n",
    "\n",
    "The Gauss-Newton method is an optimization algorithm used to solve non-linear least squares problems. It consists of three steps\n",
    "\n",
    "1. Find initial estimates of the ARMA parameters\n",
    "2. Linearize the cost function using Taylor expansion\n",
    "3. Apply the Gauss-Newton update step\n",
    "\n",
    "Since non-linear problems rarely are convex, we cannot be sure to find a global optimum if our initial parameters starts far from the correct solution. To find good initial estiamtes, either method of momemtents, Yule-Walker, or ACF/PACF can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c62d7b-89c4-493e-8a3b-e38c09e487df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "***Linearizing the cost function***\n",
    "\n",
    "To linearize the cost function, a first order taylor expansion is used.\n",
    "\n",
    "To linearize a non-linear function \\\\(w_t(\\mathbf{\\beta})\\\\) using a first order Taylor expansion, the value at a point \\\\(w_t(\\mathbf{\\beta}_k)\\\\) alongside the gradient at that point \\\\(\\frac{d}{d\\beta} w_t(\\mathbf{\\beta}_k)\\\\). The value of the function is then approximated by multiplying the gradient to the step size and subtracting it from the value \\\\(w_t(\\mathbf{\\beta}_k)\\\\).\n",
    "\n",
    "In the one dimensional case, this becomes\n",
    "\n",
    "$$ w_t(\\mathbf{\\beta}) \\approx w_t(\\mathbf{\\beta}_k) + \\delta_\\beta w_t(\\mathbf{\\beta}_k) (\\mathbf{\\beta} - \\mathbf{\\beta}_k) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c177b40c-8e8e-47d8-9f72-933ed941d2f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "## Example on linearizing a simple non-linear function\n",
    "# It is seen how an increasing number of orders of the Taylor series improves the approximation\n",
    "\n",
    "# Function\n",
    "def f(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "# First order derivative\n",
    "def f_d(x, order=1):\n",
    "    order -= 1\n",
    "    if order <= 0:\n",
    "        return f(x)\n",
    "    else:\n",
    "        return f_d(x, order)\n",
    "   \n",
    "\n",
    "y = np.linspace(-1, 1, 100)\n",
    "\n",
    "x_0 = -0.5\n",
    "x_approximations = [x_0 + i * 0.1 for i in range(10)]\n",
    "\n",
    "y_hats_first = [f(x_0) + f_d(x_0) * (x_1 - x_0) for x_1 in x_approximations]\n",
    "y_hats_second = [f(x_0) + f_d(x_0) * (x_1 - x_0) + f_d(x_0, order=2)/(math.factorial(2)) * (x_1 - x_0)**2 for x_1 in x_approximations]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y, f(y), label=\"f(x)\")\n",
    "plt.scatter(x_approximations, y_hats_first, label=\"First order approximation\")\n",
    "plt.scatter(x_approximations, y_hats_second, label=\"Second order approximation\")\n",
    "plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b5c1131-1e50-402e-a8c0-5954d4ee6436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Taylor expansion fro several dimensions**\n",
    "\n",
    "Taylor expansion on matrix form is\n",
    "\n",
    "$$ w_t(\\mathbf{\\beta}) \\approx w_t(\\mathbf{\\beta}_k) + \\nabla_\\beta w_t(\\mathbf{\\beta}_k)^T (\\mathbf{\\beta} - \\mathbf{\\beta}_k) $$\n",
    "\n",
    "Where \\\\(\\nabla_\\mathbf{\\beta} = [\\frac{d f_1}{d \\mathbf{\\beta}}, \\frac{d f_2}{d \\mathbf{\\beta}}.. \\frac{d f_n}{d \\mathbf{\\beta}}]^T\\\\)\n",
    "\n",
    "**Update step**\n",
    "\n",
    "For each succesive step towards a solution, we want to minimze the sum of squares \\\\(S(\\mathbf{\\beta})\\\\) by finding a new value of \\\\(\\mathbf{\\beta}\\\\) which makes \\\\(w_t(\\mathbf{\\beta})\\\\) equal(or close) to 0.\n",
    "\n",
    "$$ 0 \\approx w_{t}(\\mathbf{\\beta}_k) + \\nabla_\\beta w_t(\\mathbf{\\beta}_k)^T (\\mathbf{\\beta} - \\mathbf{\\beta}_k) $$\n",
    "\n",
    "We will solve this equation for values \\\\(r>max(p,q)\\\\) to \\\\(r < T+1\\\\).\n",
    "\n",
    "This is equivalent to minimizing the cost function \n",
    "\n",
    "$$ \\mathbf{S(\\mathbf{\\beta})} = || \\mathbf{r}_k + \\mathbf{J}_k(\\mathbf{\\beta} - \\mathbf{\\beta}_k) ||^2 $$\n",
    "\n",
    "Where we define\n",
    "\n",
    "$$ \\mathbf{r}_k=[ w_{r+1} (\\mathbf{\\beta}_k),..,w_T (\\mathbf{\\beta}_k) ]^T $$\n",
    "\n",
    "And the Jacobian \\\\( \\mathbf{J}_k \\in \\R^{(T-r) \\times (p+q)} \\\\) where each row is \\\\(\\nabla_\\beta w_t(\\mathbf{\\beta}_k)^T\\\\).\n",
    "\n",
    "We define \\\\(\\Delta\\mathbf{\\beta} = \\mathbf{\\beta} - \\mathbf{\\beta}_k\\\\) expand the cost function\n",
    "\n",
    "$$ \\mathbf{S(\\mathbf{\\beta})} = || \\mathbf{r}_k + \\mathbf{J}_k\\Delta\\mathbf{\\beta} ||^2 = (\\mathbf{r}_k + \\mathbf{J}_k\\Delta\\mathbf{\\beta})^T(\\mathbf{r}_k + \\mathbf{J}_k\\Delta\\mathbf{\\beta})$$\n",
    "\n",
    "Taking the derivative with respect to \\\\(\\Delta\\mathbf{\\beta}\\\\) and set to 0.\n",
    "\n",
    "$$\\frac{d}{d\\Delta\\mathbf{\\beta}} \\mathbf{S(\\mathbf{\\beta})} = 2(J_k^T J_k) \\Delta\\mathbf{\\beta} + 2 J_k^T\\mathbf{r}_k=0$$\n",
    "\n",
    "Solving for \\\\(\\Delta\\mathbf{\\beta}\\\\) yields\n",
    "\n",
    "$$\\Delta\\mathbf{\\beta} = -(J_k^T J_k)^{-1} J_k^T \\mathbf{r}_k$$\n",
    "\n",
    "Hence, the update step becomes\n",
    "\n",
    "$$\\mathbf{\\beta}_{k+1} = \\mathbf{\\beta}_k - (J_k^T J_k)^{-1} J_k^T \\mathbf{r}_k$$\n",
    "\n",
    "**Calculation of the Jacobian**\n",
    "\n",
    "Given a vector valued function\n",
    "\n",
    "$$ f(x) = [f_1(x_1,..,x_n),f_2(x_1,..,x_n),..,f_m(x_1,..,x_n),]^T $$\n",
    "\n",
    "The Jacobian is the the \\\\(m \\times n\\\\) matrix with the first order derivatives of the function \\\\(f(x)\\\\).\n",
    "\n",
    "To calculate the partial derivatives we create a small pertubation in one of the dimensions in \\\\(\\mathbf{\\beta}\\\\) to find the difference in error and then we divide it by the difference in parameters\n",
    "\n",
    "$$\\frac{d w_t}{d \\beta_k} = \\frac{w_t(\\mathbf{\\beta} + e \\mathbf{h_k}) - w_t(\\mathbf{\\beta})}{h}$$\n",
    "\n",
    "Here \\\\(e\\\\) is a very small number, and \\\\(\\mathbf{h}_k\\\\) is the unit vector with a 1 at the k'th position.\n",
    "\n",
    "This is done for all values from \\\\(t=max(p,q)+1\\\\) to \\\\(t=T\\\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505b45b3-67d2-43c2-97de-87f5a2dd35c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Helper function for estimating ARMA parameters using the Gauss-Newton method\n",
    "\n",
    "\n",
    "# Generate ARMA process\n",
    "ar_params = np.array([.75, -.25])\n",
    "ma_params = np.array([.65, .35])\n",
    "\n",
    "def gen_arma_data(num_samples, mu, sigma, ar_params, ma_params):\n",
    "\n",
    "    w = sigma * np.random.randn(num_samples) + mu\n",
    "    x = np.zeros(num_samples)   \n",
    "\n",
    "    print(f\"Generate ARMA data. AR params: {ar_params}, MA params: {ma_params}\")\n",
    "    \n",
    "    # Generate data for the model\n",
    "    # x[n] = ar_params_1 * x[n-1] + .. + ar_params_k * x[n-p] + w[n] + ma_params_1 * w[n-1] + .. + ma_params_q * w[n-q]\n",
    "    ar_ = np.r_[np.array([1]), -ar_params]\n",
    "    ma_ = np.r_[np.array([1]), ma_params]\n",
    "    x = signal.lfilter(b=ma_, a=ar_, x=w)\n",
    "    return x, w\n",
    "\n",
    "x, w = gen_arma_data(100, 0, 0.1, ar_params, ma_params)\n",
    "\n",
    "def calc_r(ar_params, ma_params):\n",
    "    p = len(ar_params)\n",
    "    q = len(ma_params)\n",
    "    r = max(p, q) + 1\n",
    "    return p, q, r\n",
    "\n",
    "def predict_arma(x, ar_params_est, ma_params_est):\n",
    "    p, q, r = calc_r(ar_params_est, ma_params_est)\n",
    "\n",
    "    x_hat = np.zeros(len(x))\n",
    "    x_hat[:r] = 0#x[:r]\n",
    "    \n",
    "    residuals = np.zeros(len(x))\n",
    "\n",
    "    for i in range(r, len(x)):\n",
    "\n",
    "        ar = np.dot(ar_params_est, x[i-p:i][::-1])\n",
    "        ma = np.dot(ma_params_est, residuals[i-q:i][::-1])\n",
    "        x_hat[i] = ar + ma \n",
    "        residuals[i] = x[i] - x_hat[i]\n",
    "    return residuals, x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "724c4c09-f4d0-4fd5-a48c-42924678ab4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Example on estimating ARMA parameters using the Gauss-Newton method\n",
    "\n",
    "def gauss_newton_update(x, ar_params_est, ma_params_est):\n",
    "\n",
    "    p, q, r = calc_r(ar_params_est, ma_params_est)\n",
    "\n",
    "    # Compute residuals\n",
    "    residuals, x_hat = predict_arma(x, ar_params_est, ma_params_est)\n",
    "\n",
    "    # Compute Jacobian\n",
    "    M = len(x)-r\n",
    "    N = p+q\n",
    "    J = np.zeros((M, N))\n",
    "\n",
    "    h = 1e-6\n",
    "\n",
    "    # For each column\n",
    "    for n in range(N):\n",
    "    \n",
    "        # Create the pertubation in parameters\n",
    "        # We only pertube one dimension of the parameters at a time\n",
    "        e = np.zeros(p+q)\n",
    "        e[n] = 1\n",
    "        ar_params_perturbed = ar_params_est + h*e[:p]\n",
    "        ma_params_perturbed = ma_params_est + h*e[p:]\n",
    "\n",
    "        # Calculate the residual value for the perturbed parameters\n",
    "        residuals_perturbed, x_hat = predict_arma(x, ar_params_perturbed, ma_params_perturbed)\n",
    "\n",
    "        # Find the change in the residual and divide it by the change in parameters\n",
    "        J[:, n] = (residuals_perturbed[r:] - residuals[r:])/h\n",
    "\n",
    "    # Update parameters using the Gauss-Newton Update step\n",
    "    delta = np.linalg.inv(J.T @ J) @ J.T @ residuals[r:]\n",
    "    ar_params_est -= delta[:p]\n",
    "    ma_params_est -= delta[p:]\n",
    "\n",
    "    residuals, x_hat = predict_arma(x, ar_params_est, ma_params_est)\n",
    "\n",
    "    return ar_params_est, ma_params_est, np.mean(residuals[r:]**2)\n",
    "\n",
    "def gauss_newton_method(x, initial_ar_params, initial_ma_params, convergence_threshold, max_iterations):\n",
    "\n",
    "    ar_params_est = initial_ar_params\n",
    "    ma_params_est = initial_ma_params\n",
    "\n",
    "    n = 0\n",
    "    residuals_sum_of_squares_ = []\n",
    "    while True:\n",
    "\n",
    "        # Try to perform the update step - This might fail if the Jacobian is ill conditioned\n",
    "        try:\n",
    "            ar_params_est, ma_params_est, residuals_sum_of_squares = gauss_newton_update(x, ar_params_est, ma_params_est)\n",
    "            residuals_sum_of_squares_ += [residuals_sum_of_squares]\n",
    "        except:\n",
    "            return ar_params_est, ma_params_est, residuals_sum_of_squares_, False\n",
    "        \n",
    "        # If more than one iteration has been performed, check for convergence\n",
    "        if len(residuals_sum_of_squares_) > 1:\n",
    "            # Check that the change in the sum of squares is less than the convergence threshold\n",
    "            convergence_percentage = np.abs(residuals_sum_of_squares_[-1] - residuals_sum_of_squares_[-2]) / residuals_sum_of_squares_[-1]\n",
    "            if convergence_percentage < convergence_threshold:\n",
    "                return ar_params_est, ma_params_est, residuals_sum_of_squares_, True\n",
    "\n",
    "        n += 1\n",
    "\n",
    "        if n > max_iterations:\n",
    "            return ar_params_est, ma_params_est, residuals_sum_of_squares_, False\n",
    "\n",
    "mu, sigma = 0, 0.1\n",
    "x, w = gen_arma_data(10000, mu, sigma, ar_params, ma_params)\n",
    "\n",
    "convergence_threshold = 1e-10\n",
    "\n",
    "# Use initial estimates which are not far from the true ones\n",
    "# Since non-linear optimization problems rarely are convex, it is important to start from a good initial guess\n",
    "initial_ar_params = ar_params + np.random.randn(len(ar_params)) * 0.5\n",
    "initial_ma_params = ma_params + np.random.randn(len(ma_params)) * 0.5\n",
    "\n",
    "print(f\"initial_ar_params: {initial_ar_params}, initial_ma_params: {initial_ma_params}\")\n",
    "\n",
    "ar_params_est, ma_params_est, residuals_sum_of_squares_, converged = gauss_newton_method(x, \n",
    "                                                                                         initial_ar_params, \n",
    "                                                                                         initial_ma_params, \n",
    "                                                                                         convergence_threshold=convergence_threshold, \n",
    "                                                                                         max_iterations=20)\n",
    "\n",
    "print(f\"True params\")\n",
    "print(ar_params, ma_params)\n",
    "print(f\"Final params\")\n",
    "print(ar_params_est, ma_params_est)\n",
    "\n",
    "plt.figure()   \n",
    "plt.plot(residuals_sum_of_squares_, '*-', label=\"Residuals sum of squares\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Residuals sum of squared\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "\n",
    "residuals, x_hat = predict_arma(x, ar_params_est, ma_params_est)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x[:100], label=\"Signal\")\n",
    "plt.plot(x_hat[:100], label=\"Estimated signal\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(w[:100], label=\"Noise\")\n",
    "plt.plot(residuals[:100], label=\"Residuals\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ARMA Parameter Estimation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
