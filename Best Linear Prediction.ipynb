{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "765db622-61b0-49cc-a9bd-e5afcc0fdaba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Best Linear Prediction**\n",
    "\n",
    "In best linear prediction to goal is to predict a value \\\\(y\\\\) from previous values \\\\(x_1\\\\), \\\\(x_2\\\\) etc. \n",
    "\n",
    "When using best linear predcition in time series, we try to predict a future value from historical values.\n",
    "\n",
    "To ensure our prediction is the \"best\", the mean squared error \\\\(E[x_{n+m}-\\widehat{x}_{n+m}]\\\\) is minimized. \n",
    "\n",
    "To guarantee that the MSE is minimized, the orthogonality principle is used. This states that the prediction error must be uncorrelated with the predictor variable. Mathematically, this is stated as\n",
    "\n",
    "$$ cov(x_{n+m}-\\widehat{x}_{n+m}, x_k) = E[(x_{n+m}-\\widehat{x}_{n+m})x_k] = 0 $$\n",
    "\n",
    "For \\\\(k=1,2,3,..,n\\\\)\n",
    "\n",
    "Considering the case of one step prediction where \\\\(x_0, x_1,...,x_n\\\\) is given and we want to find \\\\(x_{n+1}\\\\)\n",
    "\n",
    "$$ \\widehat{x}_{n+1} = \\phi_{n,1} x_n + \\phi_{n,2} x_{n-2} + ... + \\phi_{n,n} x_{1} = \\sum_{j=1}^n \\phi_{n,j} x_{n+1-j}$$ \n",
    "\n",
    "Here the coeffecients are denoted \\\\(\\phi_{i,j}\\\\) to specify that they are coeffecient \\\\(j\\\\) calculated with samples available at time \\\\(i\\\\).\n",
    "\n",
    "This leads to the following optimality criterion.\n",
    "\n",
    "$$ E[(x_{n+1}-\\widehat{x}_{n+1})x_{n+1-k}] = 0$$\n",
    "\n",
    "For \\\\(k=1,2,3,..,n\\\\)\n",
    "\n",
    "Inserting \\\\(\\widehat{x}\\\\)\n",
    "\n",
    "$$ E[(x_{n+1}-\\sum_{j=1}^n \\phi_{n,j} x_{n+1-j})x_{n+1-k}] = 0 $$\n",
    "\n",
    "$$ E[x_{n+1}x_{n+1-k}]-\\sum_{j=1}^n \\phi_{n,j} E[x_{n+1-j}x_{n+1-k}] = 0 $$\n",
    "\n",
    "$$ \\sum_{j=1}^n \\phi_{n,j} \\gamma(k-j) = \\gamma(k) $$\n",
    "\n",
    "This can be written in matrix form\n",
    "\n",
    "$$ \\mathbf{\\Gamma_n} \\mathbf{\\phi}_n = \\mathbf{\\gamma_n} $$\n",
    "\n",
    "Where \\\\(\\mathbf{\\Gamma_n}\\\\) is the \\\\(n \\times n\\\\) autocovariance matrix with values from \\\\(\\gamma(0)\\\\) to \\\\(\\gamma(n-1)\\\\) and \\\\(\\mathbf{\\gamma_n}\\\\) is the \\\\(n \\times 1\\\\) vector with values \\\\(\\gamma(1)\\\\) to \\\\(\\gamma(n)\\\\). \n",
    "\n",
    "To estimate a model with 2 parameters, this will yield the two following equations\n",
    "\n",
    "$$ \\phi_{n,1} \\gamma(-1) + \\phi_{n,2} \\gamma(0) = \\gamma(1) $$\n",
    "\n",
    "$$ \\phi_{n,1} \\gamma(0) + \\phi_{n,2} \\gamma(1) = \\gamma(2) $$\n",
    "'\n",
    "Notice how we're trying to predict the k'th covariance from the two previous. Intuitively this makes sense, since we want to \n",
    "\n",
    "solving for the coeffecients yields\n",
    "\n",
    "$$ \\mathbf{\\phi}_n = \\mathbf{\\Gamma_n}^{-1} \\mathbf{\\gamma_n} $$\n",
    "\n",
    "The one step prediction in matrix for is\n",
    "\n",
    "$$ \\widehat{x}_{n+1} = \\bold{\\phi}_n^T \\bold{x} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6e0e6d6-f9b1-4f0d-9114-71e7d389f33a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Define autocovariance\n",
    "def autocovariance(x, lag):\n",
    "    n = len(x)\n",
    "    mean_x = np.mean(x)\n",
    "    return np.sum((x[:n - lag] - mean_x) * (x[lag:] - mean_x)) / n\n",
    "\n",
    "## Define autocovariance matrix\n",
    "def autocov_matrix(x, max_lag):\n",
    "    return np.array([[autocovariance(x, abs(i - j)) for j in range(max_lag + 1)] for i in range(max_lag + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7672e90b-76fc-4585-b430-fde8c5739e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "N = 10000\n",
    "mu, sigma = 0, 0.1\n",
    "phi = [1.5, -0.75, 0.5, -0.3]\n",
    "# We know the number of model coeficients\n",
    "num_model_coefs = len(phi)\n",
    "\n",
    "def gen_blp_data(num_samples, sigma, mu, phi):\n",
    "    w = sigma * np.random.randn(num_samples) + mu\n",
    "    x = np.zeros(num_samples)   \n",
    "    \n",
    "    # Generate data for the model\n",
    "    # x[n] = phi_1 * x[n-1] + phi_2 * x[n-2] + .. + phi_k * x[n-k] + w[n]\n",
    "    x = signal.lfilter(b=[1], a=[1] + [-phi_ for phi_ in phi], x=w)\n",
    "    return x, w\n",
    "\n",
    "def calc_blp_coefs(x, num_model_coefs):\n",
    "    cov = autocov_matrix(x, num_model_coefs)\n",
    "    # Solve the equation Ax=b to find the coefficients\n",
    "    Gamma = cov[-num_model_coefs:, -num_model_coefs:]\n",
    "    gamma = cov[1:, 0]\n",
    "    coefs = np.linalg.inv(Gamma) @ gamma\n",
    "    return coefs, cov, Gamma, gamma\n",
    "\n",
    "def blp_predict(x, prediction_coefs):\n",
    "\n",
    "    x_hat = np.zeros_like(x)\n",
    "    num_prediction_coefs = len(prediction_coefs)\n",
    "    for i in range(num_prediction_coefs , len(x)):\n",
    "        #x_hat[i] = x[i-1] * prediction_coefs[0] + x[i-2] * prediction_coefs[1] + x[i-3] * prediction_coefs[2] + x[i-4] * prediction_coefs[3]\n",
    "        for n in range(num_prediction_coefs):\n",
    "            x_hat[i] += x[i-(n+1)] * prediction_coefs[n]\n",
    "\n",
    "    # only calculate MSE on valid predictions. The first num_prediction_coefs prediction are 0, since we need that number of \n",
    "    # samples to predict the next sample.\n",
    "    mse = np.mean((x[num_prediction_coefs:] - x_hat[num_prediction_coefs:])**2)\n",
    "    return x_hat, mse\n",
    "\n",
    "    \n",
    "x, w = gen_blp_data(N, sigma, mu, phi)\n",
    "\n",
    "coefs, cov, Gamma, gamma = calc_blp_coefs(x, num_model_coefs)\n",
    "\n",
    "x_hat, mse = blp_predict(x, coefs)\n",
    "\n",
    "print(f\"Gamma: \\n{Gamma}\")\n",
    "print(f\"gamma: \\n{gamma}\")\n",
    "print(f\"Coefs: {coefs}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x[:100], label=\"x\")\n",
    "plt.plot(x_hat[:100], label=\"x estimated\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"samples\")\n",
    "plt.ylabel(\"Value\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(w[:100], label=\"w\")\n",
    "plt.plot(x[:100] - x_hat[:100], label=\"Estimate error\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"samples\")\n",
    "plt.ylabel(\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee50ba18-43e0-4393-934e-f0b36b86de8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Prediction Error**\n",
    "\n",
    "The mean square one-step prediction error is defined as \n",
    "\n",
    "$$ P_{n+1} = E[(x_{n+1}-\\widehat{x}_{n+1})^2] = E[(x_{n+1}-\\bold{\\phi}_n^T \\bold{x})^2]$$\n",
    "\n",
    "By inserting \\\\(\\mathbf{\\phi}_n = \\mathbf{\\Gamma_n}^{-1} \\mathbf{\\gamma_n}\\\\) we get\n",
    "\n",
    "$$ P_{n+1} = E[(x_{n+1}-(\\mathbf{\\Gamma_n}^{-1} \\mathbf{\\gamma_n})^T \\bold{x})^2] = E[(x_{n+1}-\\mathbf{\\gamma_n}^T\\mathbf{\\Gamma_n}^{-1} \\bold{x})^2]$$\n",
    "\n",
    "Since \\\\((\\bold{A}\\bold{x})^T=\\bold{x}^T\\bold{A}^T\\\\) and \\\\(\\mathbf{\\Gamma}_n^{-1}\\\\) is symmetric.\n",
    "\n",
    "$$ E[(x_{n+1}-\\mathbf{\\gamma_n}^T\\mathbf{\\Gamma_n}^{-1} \\bold{x})^2] = E[x_{n+1}^2] - 2 E[\\mathbf{\\gamma_n}^T\\mathbf{\\Gamma_n}^{-1} \\bold{x} x_{n+1}] + E[\\mathbf{\\gamma_n}^T\\mathbf{\\Gamma_n}^{-1} \\bold{x} \\bold{x}^T \\mathbf{\\Gamma_n}^{-1} \\mathbf{\\gamma_n}]$$\n",
    "\n",
    "And since\n",
    "\n",
    "$$E[\\bold{x} \\bold{x}^T]=\\mathbf{\\Gamma_n}$$\n",
    "\n",
    "$$E[\\bold{x}x_{n+1}]=\\gamma_{n}$$\n",
    "\n",
    "$$E[x_{n+1}^2]= \\gamma_n(0)$$\n",
    "We can simplify to\n",
    "\n",
    "$$ E[(x_{n+1}-\\mathbf{\\gamma_n}^T\\mathbf{\\Gamma_n}^{-1} \\bold{x})^2] = E[x_{n+1}^2] - 2 \\mathbf{\\gamma_n}^T\\mathbf{\\Gamma_n}^{-1} \\gamma_n + \\mathbf{\\gamma_n}^T\\mathbf{\\Gamma_n}^{-1} \\mathbf{\\Gamma_n} \\mathbf{\\Gamma_n}^{-1} \\mathbf{\\gamma_n} = \\gamma_n(0) - \\mathbf{\\gamma_n}^T\\mathbf{\\Gamma_n}^{-1} \\gamma_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50b0d37e-dcf8-41ab-b5e6-4f32a1587688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Example on mean squared error of best linear prediction\n",
    "# The empirical MSE converges towards the theoretical MSE as the number of samples increases\n",
    "\n",
    "num_samples_test = [2**i for i in range(8, 16)]\n",
    "\n",
    "def blp_theoretical_mse(cov, Gamma, gamma):\n",
    "    \n",
    "    return cov[0][0] - gamma.T @ np.linalg.inv(Gamma) @ gamma\n",
    "\n",
    "theoretical_mses = []\n",
    "empirical_mses = []\n",
    "\n",
    "for num_samples in num_samples_test:\n",
    "    x, w = gen_blp_data(num_samples, sigma, mu, phi)\n",
    "\n",
    "    coefs, cov, Gamma, gamma = calc_blp_coefs(x, num_model_coefs)\n",
    "    theoretical_mses += [blp_theoretical_mse(cov, Gamma, gamma)]\n",
    "    x_hat, mse = blp_predict(x, coefs)\n",
    "    empirical_mses += [mse]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_samples_test, theoretical_mses, label=\"Theoretical mse\")\n",
    "plt.plot(num_samples_test, empirical_mses, label=\"Empirical mse\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE of best linear prediction\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Best Linear Prediction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
