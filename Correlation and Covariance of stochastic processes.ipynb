{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "581cec48-7ca2-4bb0-a34c-bb9b91e9d022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2a8fdfa-3df0-4c92-8902-319839be4252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Definition - Autocovariance**\n",
    "\n",
    "The autocovariance function is defined as the second moment product\n",
    "\n",
    "\\\\(cov(x{_s},x{_t})=E[(x{_s}-\\mu{_s})(x{_t}-\\mu{_t})]=E[x{_s}x{_t}]-E[x{_s}]E[x{_t}]\\\\)\n",
    "\n",
    "This measures the linear dependence between two points on the same series observed at time s and t respectively.\n",
    "\n",
    "If \\\\(s = t\\\\) then \\\\(E[x{_s}x{_t}]=E[x{_s}^2]\\\\) otherwise \\\\(E[x{_s}x{_t}]=E[x{_s}]E[x{_t}]\\\\).\n",
    "\n",
    "Hence, the autocovariance if s is not equal t is 0.\n",
    "\n",
    "If \\\\(s=t\\\\) then \\\\(E[x{_s}x{_d}]=E[x{_s}^2]=\\sigma{_x}^2+\\mu{_x}^2 \\\\) and the covariance will be \\\\(cov(x{_s},x{_s})=\\sigma{_x}^2+\\mu{_x}^2-\\mu{_x}^2=\\sigma{_x}^2\\\\)\n",
    "\n",
    "Otherwise if \\\\(s\\\\) not equal \\\\(t\\\\) the covariance will be \\\\(cov(x{_s},x{_t})=E[x{_s}]E[x{_t}]-E[x{_s}]E[x{_t}]=0\\\\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ac4957-1190-4976-9f52-596c4865f075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Definition - Sample Autocovariance function**\n",
    "\n",
    "The sample autocovariance function is defined as \n",
    "\n",
    "$$\\hat \\gamma (h) = \\frac{1}{N}\\sum_{t=1}^{N-h}(x_\\{t+h}-\\overline x)(x_\\{t}-\\overline x)$$\n",
    "\n",
    "Notice how the correction terma is \\\\(\\frac{1}{N}\\\\) and not \\\\(\\frac{1}{N-h}\\\\). Taking a look at the expected value of the sample autocovariance function\n",
    "\n",
    "$$E[\\hat \\gamma (h)]=\\frac{1}{N} \\sum_{i=1}^{N-\\left|h\\right|} \\gamma(h)=\\frac{1}{N}(N-\\left|h\\right|)\\gamma(h)=(1-\\frac{\\left|h\\right|}{N})\\gamma(h)$$\n",
    "\n",
    "It can be seen that is a biased estimator. The reason for not using the correction term, is that we want to ensure that the covariance and future covariance matrices are positive semi-definite. However, as N increased, the biases goes to zero, so for a sufficiently large sample size, this does not have any practical effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7b9784-68be-4443-9859-e2bc2072875c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Here the autocovariance is defined\n",
    "def autocovariance(signal, lag):\n",
    "    mean = np.mean(signal)\n",
    "    n = len(signal)\n",
    "    return np.mean((signal[:n-lag] - mean) * (signal[lag:] - mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd12cea0-a5a3-4168-bea6-09afb2ad1743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Example - Autocovariance of white noise**\n",
    "\n",
    "Let \\\\(w_t\\\\) be a time series consisting of white noise with mean \\\\(\\mu{_w}\\\\) and \\\\(var(w)=\\sigma^2{_w}\\\\). The white noise will have an autocovariance \\\\(\\gamma{_w}(s,t)=cov(w{_s},w{_t})\\\\) with value \\\\(\\sigma^2{_w}\\\\) when \\\\(s=t\\\\) and \\\\(0\\\\) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b74934-ddc6-41ad-a244-622ca9daa5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## Here a simulation of the autocovariance is made with different means and a standard deviation of 3\n",
    "## to show that the autocovariance of white noise i 0 when s != t and sigma^2 when s=t.\n",
    "\n",
    "## num_samples is the number of samples to be simulated\n",
    "num_samples = [100*i for i in range(1000)]\n",
    "\n",
    "## Simulate the autocovariance with different number of samples\n",
    "def simulate_autocovariance(mu, sigma, num_samples):\n",
    "\n",
    "    covs = np.zeros(len(num_samples))\n",
    "    covs_lagged = np.zeros(len(num_samples))\n",
    "    for n, samples in enumerate(num_samples):\n",
    "        mu, sigma = 2, 3 # mean = 2 and standard deviation = 3\n",
    "        w = np.random.normal(mu, sigma, samples)\n",
    "        w_lagged = w[1:]\n",
    "        w = w[:-1]\n",
    "\n",
    "        # Calculate autocovariance for w with itself\n",
    "        cov_w = autocovariance(w, 0)\n",
    "        cov_w_lagged = autocovariance(w, 1)\n",
    "        covs[n] = cov_w\n",
    "        covs_lagged[n] = cov_w_lagged\n",
    "\n",
    "    return covs, covs_lagged\n",
    "\n",
    "## Calculate autocovariance with means 0 and 2 with standard deviation 3\n",
    "cov_w_mean_2, cov_w_mean_2_lagged = simulate_autocovariance(2, 3, num_samples)\n",
    "cov_w_mean_0, cov_w_mean_0_lagged = simulate_autocovariance(0, 3, num_samples)\n",
    "\n",
    "## Plot how the autocovariance converges towards the theoretical value when the number of samples are large enough\n",
    "plt.figure()\n",
    "plt.plot(num_samples, cov_w_mean_2, label=\"Autocovariance s equal t (mean = 2)\")\n",
    "plt.plot(num_samples, cov_w_mean_0, label=\"Autocovariance s equal t (mean = 0)\")\n",
    "plt.legend()\n",
    "plt.title(\"Autocovariance for s=t\")\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Autocovariance\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_samples, cov_w_mean_2_lagged, label=\"Autocovariance s not equal t (mean = 2)\")\n",
    "plt.plot(num_samples, cov_w_mean_0_lagged, label=\"Autocovariance s not equal t (mean = 0)\")\n",
    "plt.legend()\n",
    "plt.title(\"Autocovariance for t=s+1\")\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Autocovariance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1759e15-03f8-47b4-8131-b5375599d936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Example - Autocovariance of a moving average**\n",
    "\n",
    "A new signal \\\\(v_{s}\\\\) is constructed by applying a two point average to a white noise series $$v{_s}=\\frac{1}{2}(w{_s}+w_\\{s-1})$$\n",
    "\n",
    "Then the autocovariance function becomes \\\\(\\gamma{_v}(s,t)=cov(\\frac{1}{2}(w{_s}+w_\\{s-1}, \\frac{1}{2}(w{_t}+w_\\{t-1})\\\\).\n",
    "\n",
    "When \\\\(t=s\\\\) then $$\\gamma{_v}(s,s)=cov(\\frac{1}{2}(w{_s}+w_\\{s-1}), \\frac{1}{2}(w{_s}+w_\\{s-1}))=\\frac{1}{4}(cov(w_s, w_s)+cov(w_{s-1}, w_s)+cov(w_s, w_{s-1})+cov(w_\\{s-1}, w_\\{s-1}))=\\frac{1}{2}\\sigma{_w}^2$$\n",
    "\n",
    "These simplifications can be made since the random variables \\\\(U=\\sum_j a_{j}X_{j}\\\\) and \\\\(V=\\sum_k b_{k}Y_{k}\\\\) are linear combinations of the random variables \\\\(X_{j}\\\\) and \\\\(Y_{k}\\\\) and will have \\\\(cov(U,V)=\\sum_j \\sum_k a{_j}b{_k}cov(X{_j},Y{_k})\\\\).\n",
    "\n",
    "When \\\\(t=s-1\\\\) then $$\\gamma{_v}(s,s+1)=cov(\\frac{1}{2}(w{_s}+w_\\{s-1}), \\frac{1}{2}(w_\\{s-1}+w_\\{s-2}))=\\frac{1}{4}\\sigma{_w}^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba47d177-1e8e-4a10-91d6-332879f7f897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mu, sigma = 0, 3 # mean = 0 and standard deviation = 3\n",
    "## Use a sample figure large enough to show that the autocovariance converges to the theoretical value\n",
    "num_samples = 100000\n",
    "\n",
    "# Create the white noise signal\n",
    "w = np.random.normal(mu, sigma, num_samples)\n",
    "\n",
    "# Create the moving average signal\n",
    "wf = np.zeros(num_samples)\n",
    "for i in range(1, num_samples):\n",
    "    wf[i] = 1/2*(w[i] + w[i-1])\n",
    "\n",
    "# Create the lagged white noise signals\n",
    "wf_lagged_one = wf[1:-1]\n",
    "wf_lagged_two = wf[2:]\n",
    "wf = wf[:-2]\n",
    "\n",
    "print(f\"Autocovariance of signal wf with lag = 0: Autocovariance = {autocovariance(wf, 0):.3f}, Theoretical autocovariance = {1/2*sigma**2}\")\n",
    "print(f\"Autocovariance of signal wf with lag = 1: Autocovariance = {autocovariance(wf, 1):.3f}, Theoretical autocovariance = {1/4*sigma**2}\")\n",
    "print(f\"Autocovariance of signal wf with lag = 2: Autocovariance = {autocovariance(wf, 2):.3f}, Theoretical autocovariance = {0}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d32b6b-8ff2-435b-818a-6b4d40e4b7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Example - Autocovariance of a Random Walk**\n",
    "\n",
    "The random walf is defined as $$x_{t}=\\delta t+\\sum_\\{j=1}^sw_\\{j}$$ \n",
    "\n",
    "Here \\\\(\\delta\\\\) is the drift in time. For this example this is set to 0.\n",
    "\n",
    "The autocovariance is then \n",
    "\n",
    "$$\\gamma(s,t)=cov(x_{s},x_{t})=cov(\\sum_\\{j=1}^sw_\\{j}, \\sum_\\{k=1}^tw_\\{k})$$\n",
    "\n",
    "Rewriting using the definition of autocovariance this becomes:\n",
    "\n",
    "$$cov(\\sum_\\{j=1}^sw_\\{j}, \\sum_\\{k=1}^tw_\\{k})=E[\\sum_\\{j=1}^sw_\\{j}\\sum_\\{k=1}^tw_\\{k}]=E[\\sum_\\{j=1}^s \\sum_\\{k=1}^t w_\\{j}w_\\{k}]]=\\sum_\\{j=1}^s \\sum_\\{k=1}^t E[w_\\{j}w_\\{k}]]$$\n",
    "\n",
    "To simplyfy this, consider the case where \\\\(s=2\\\\) and \\\\(t=3\\\\)\n",
    "\n",
    "$$\\gamma(2,3)=E[w_{1}w_{1} + w_{1}w_{2} + w_{1}w_{3} + w_{2}w_{1} + w_{2}w_{2} + w_{2}w_{3}]$$\n",
    "\n",
    "Since \\\\(E[w_{j}w_{k}]=0\\\\) when \\\\(j\\\\) is not equal to \\\\(k\\\\) this is reduced to:\n",
    "\n",
    "$$\\gamma(2,3)=E[w_{1}w_{1} + w_{2}w_{2}]=2\\sigma_{w}^2$$\n",
    "\n",
    "This can be done for different values of s and t to convince us that: \n",
    "\n",
    "$$cov(\\sum_\\{j=1}^sw_\\{j}, \\sum_\\{k=1}^tw_\\{k})=\\min(s,t) \\sigma_{w}^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d9a8d1-478c-4a40-a6bf-0bc78f90481e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mu, sigma = 0, 1 # mean = 0 and standard deviation = 3\n",
    "## Use a sample figure large enough to show that the autocovariance converges to the theoretical value\n",
    "num_samples = 100\n",
    "\n",
    "num_tests = 1000\n",
    "covariance_sum = 0\n",
    "rwalks = []\n",
    "\n",
    "for t in range(num_tests):\n",
    "    #print(f\"Test {t+1} of {num_tests}\")\n",
    "    ## Create random samples\n",
    "    w = np.random.normal(mu, sigma, num_samples)\n",
    "    ## Create the random walk by performing a cumulative sum\n",
    "    rw = np.cumsum(w)\n",
    "    rwalks += [rw]\n",
    "    ## Calculate the covariance\n",
    "    \n",
    "    cov = autocovariance(rw, 0)\n",
    "\n",
    "    covariance_sum += cov\n",
    "\n",
    "cov = covariance_sum / num_tests\n",
    "\n",
    "cov_theoretical = (num_samples)*(sigma**2)\n",
    "\n",
    "## The empirical value is only calculated with a lag of 0, which is the variance for time index t\n",
    "## This is because the random walk is non-stationary and computing the autocovariance will no yield the expected result\n",
    "cov_empirical = np.var([rw[-1] for rw in rwalks])\n",
    "print(f\"Empirical value: {cov_empirical:.3f}, theoretical value: {cov_theoretical}, error_margin: {(np.abs(cov_theoretical-cov_empirical)/cov_theoretical)*100:.3f}%\")\n",
    "\n",
    "plt.figure()\n",
    "for rw in rwalks:\n",
    "    plt.plot(rw)\n",
    "plt.title(\"Random walks\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf2f093c-ff42-4daa-bd6d-a3b51a53fba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Definition - Autocorrelation**\n",
    "\n",
    "The autocorrelation is defined as $$\\rho(s,t)=\\frac{\\gamma(s,t)}{ \\sqrt{\\mathstrut \\gamma(s,s)\\gamma(t,t)}}$$\n",
    "\n",
    "This yields a number between -1 and 1 of how well a series \\\\(x_{t}\\\\) can be linearly predicted at time \\\\(t\\\\) using only \\\\(x_s\\\\).\n",
    "\n",
    "If \\\\(x_{t}\\\\) can be perfectly predicted from \\\\(x_{s}\\\\) from the linear relationship \\\\(x_{t} = \\alpha + \\beta x_{s} \\\\) then the autocorrelation will be \\\\(+1\\\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3effa8f-a63d-427d-8691-55a2a4108e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Definition - Sampe Autocorrelation function**\n",
    "\n",
    "The sample autocovariance function is defined as \n",
    "\n",
    "$$\\hat \\rho (h) = \\frac{\\hat \\gamma (h)}{\\hat \\gamma (0)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b910703-369c-4b0b-8592-2314a32e0e11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def autocorrelation(s, lag):\n",
    "    return autocovariance(s, lag) / autocovariance(s, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79f0435-0117-4da8-a762-8bfe1a4d96ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Property - Large sample distribution of the Autocorrelation Function**\n",
    "\n",
    "A white noise process \\\\(x_{t}\\\\) is strictly stationary and will have a autocorrelation function \\\\(\\hat \\rho_{x}(h)\\\\) that is normally distributed with zero mean for \\\\(h>1\\\\) when n is large. The standard deviation is given by\n",
    "\n",
    "$$\\sigma_\\{\\rho_{x}(h)}=\\frac{1}{\\sqrt{\\mathstrut n}}$$\n",
    "\n",
    "This property allows us to asses if a peak in the autocorrelation function is significant. If we only consider peaks within \\\\(\\pm 2\\hat \\rho_{x}(h)\\\\) approximately 95% of all peaks should be within that interval, if the process is indeed white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81bb25eb-6b1f-470d-a250-ff7f082983e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Here the variance of the autocorrelation is estimated for white noise and a correlated signal and compared to the theoretical value\n",
    "## It is shown that as n grows, the correlated signal will have a variance greater than the theoretical value 1/sqrt(n)\n",
    "num_samples = [100*i for i in range(1, 20)]\n",
    "\n",
    "tests = 100\n",
    "mu, sigma = 0, 1\n",
    "lag = 2\n",
    "\n",
    "def gen_white_noise_signal(num):\n",
    "    return np.random.normal(0, 1, num)\n",
    "\n",
    "def gen_correlated_signal(num):\n",
    "    filter_length = 4\n",
    "    # Since param valid is chosen for the convolution num - filter_length + 1\n",
    "    # samples are returned. To correct for this, num+filter_length-1 samples are generated\n",
    "    w = gen_white_noise_signal(num+filter_length-1)\n",
    "    wf = np.convolve(w, np.ones(filter_length), 'valid') / filter_length\n",
    "    return wf\n",
    "\n",
    "def calculate_acf_variance(x_gen, lag, num_samples, tests):\n",
    "\n",
    "    acf_estimated_mean = []\n",
    "    acf_estimated_std = []\n",
    "\n",
    "    for n, num in enumerate(num_samples):\n",
    "\n",
    "        processes = [x_gen(num) for i in range(tests)]\n",
    "        acf_results = [autocorrelation(p, lag) for p in processes]\n",
    "\n",
    "        acf_estimated_mean += [np.mean(acf_results)]\n",
    "        acf_estimated_std += [np.std(acf_results)]\n",
    "\n",
    "    return acf_estimated_mean, acf_estimated_std\n",
    "\n",
    "white_noise_variance_theoretical = [1/np.sqrt(num) for num in num_samples]\n",
    "white_noise_variance_confidence = [2/np.sqrt(num) for num in num_samples]\n",
    "white_noise_mean_empirical, white_noise_std_empirical = calculate_acf_variance(gen_white_noise_signal, lag, num_samples, tests)\n",
    "correlated_signal_mean, correlated_signal_std = calculate_acf_variance(gen_correlated_signal, lag, num_samples, tests)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_samples, white_noise_std_empirical, label=f\"Empirical standard deviation\")\n",
    "plt.plot(num_samples, white_noise_variance_theoretical, label=\"Theoretical standard deviation\")\n",
    "plt.plot(num_samples, white_noise_variance_confidence, label=\"95% confidence standard deviation\")\n",
    "plt.title(f\"Standard deviation of estimated ACF lag={lag} of white noise\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Standard deviation of ACF\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_samples, correlated_signal_mean, label=f\"Mean ACF lag={lag} of correlated signal\")\n",
    "plt.plot(num_samples, white_noise_variance_confidence, label=\"95% confidence standard deviation ACF of white noise\")\n",
    "plt.title(f\"Correlated signal ACF lag={lag} vs 95% confidence\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Standard deviation of ACF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8402bcfc-f727-499d-8943-7446b0a28d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Definition - Cross Covariance function**\n",
    "\n",
    "The cross covariance function between two series is given as\n",
    "\n",
    "$$\\gamma_\\{xy}(h)=cov(x_\\{t+h}, y_{t})=E[(x_\\{t+h}-\\mu_{x})(y_\\{t}-\\mu_{y})]$$\n",
    "\n",
    "**Definition - Jointly stationary series**\n",
    "\n",
    "If two series are jointly stationary if they both are stationary and the  cross covariance function is only a function of lag \\\\(h\\\\).\n",
    "\n",
    "**Definition - Cross correlation function**\n",
    "\n",
    "For two jointly stationary series the cross correlation function is defined as \n",
    "\n",
    "$$\\rho_\\{xy}(h)=\\frac{\\gamma_\\{xy}(h)}{\\sqrt{\\mathstrut \\gamma_\\{x}(0) \\gamma_\\{y}(0)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8df165ea-f276-4276-8634-cba8c769964b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Definition - Sample cross covariance function**\n",
    "\n",
    "Given a realization of a random process the cross covariance can be estimated with the sample cross covariance function\n",
    "\n",
    "$$\\hat \\gamma_\\{xy}(h)=\\frac{1}{n} \\sum_\\{t=1}^\\{n-h}(x_\\{t+h}-\\overline x)(y_\\{t}-\\overline y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25559e15-370d-436d-9bb3-08cdef21a6ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Here the sample crosscovariance is defined\n",
    "def crosscovariance(x, y, lag):\n",
    "    x_mean = np.mean(x)\n",
    "    x_n = len(x)\n",
    "    y_mean = np.mean(y)\n",
    "    return np.mean((x[:x_n-lag] - x_mean) * (y[lag:] - y_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cdf73b0-5cd8-473a-b014-810ae02b7f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Definition - Sample cross correlation function**\n",
    "\n",
    "For two jointly stationary series the cross correlation function is defined as\n",
    "\n",
    "$$\\hat \\rho_\\{xy}(h)=\\frac{\\hat \\gamma_\\{xy}(h)}{\\sqrt{\\mathstrut \\hat  \\gamma_\\{x}(0) \\hat  \\gamma_\\{y}(0)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c4c18a7-4dcd-449c-8df9-9e55acc43379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Sample cross correlation\n",
    "def cross_correlation(x, y, lag):\n",
    "    return crosscovariance(x, y, lag) / np.sqrt(autocovariance(x, 0)*autocovariance(y, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4752eb5a-9931-4c52-afc2-d31f08d05a45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Property - Large sample distribution of cross correlation under independence**\n",
    "\n",
    "The large sample distribution of \\\\(\\hat \\rho_\\{xy}(h)\\\\) is normal with zero mean and $$\\sigma_\\{\\hat \\rho_\\{xy}}=\\frac{1}{\\sqrt n}$$\n",
    "\n",
    "If at least one of the series is white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4dee096-6005-4931-9722-e659d661024d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## To show that the large sample distribution of the cross correlation converges towards the theoretical value \n",
    "## an correlated and an uncorrelated signal is generated and the cross correlation is calculated\n",
    "\n",
    "num_samples = [100*i for i in range(1, 20)]\n",
    "\n",
    "tests = 100\n",
    "mu, sigma = 0, 1\n",
    "lag = 2\n",
    "\n",
    "def calculate_ccf_variance(x_gen, y_gen, lag, num_samples, tests):\n",
    "\n",
    "    ccf_estimated_mean = []\n",
    "    ccf_estimated_std = []\n",
    "\n",
    "    for n, num in enumerate(num_samples):\n",
    "\n",
    "        x_processes = [x_gen(num) for i in range(tests)]\n",
    "        y_processes = [y_gen(num) for i in range(tests)]\n",
    "        ccf_results = [cross_correlation(x, y, lag) for x, y in zip(x_processes, y_processes)]\n",
    "\n",
    "        ccf_estimated_mean += [np.mean(ccf_results)]\n",
    "        ccf_estimated_std += [np.std(ccf_results)]\n",
    "\n",
    "    return ccf_estimated_mean, ccf_estimated_std\n",
    "\n",
    "ccf_variance_theoretical = [1/np.sqrt(num) for num in num_samples]\n",
    "ccf_confidence = [2/np.sqrt(num) for num in num_samples]\n",
    "ccf_mean, ccf_std = calculate_ccf_variance(gen_white_noise_signal, gen_correlated_signal, lag, num_samples, tests)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_samples, ccf_std, label=f\"Empirical standard deviation\")\n",
    "plt.plot(num_samples, ccf_variance_theoretical, label=\"Theoretical standard deviation\")\n",
    "plt.plot(num_samples, ccf_confidence, label=\"95% confidence standard deviation\")\n",
    "plt.title(f\"Cross covariance for white noise and a correlated signal at lag={lag}\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Standard deviation of ACF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01324269-b7a3-4884-bf11-ff18275bb4cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Example - A simulated time series**\n",
    "\n",
    "Let \\\\(x_{t}\\\\) be a white noise process and \n",
    "\n",
    "$$y_{t}=5+x_{t}-0.7x_\\{t-1}$$\n",
    "\n",
    "The sample autocorrelation for the series \\\\(y_{t}\\\\) can be calculated with the sample autocorrelation function as shown before. The theoretical autocorrelation function for \\\\(h=1\\\\) can be calculated as\n",
    "\n",
    "$$\\rho_{y}(1)=\\frac{\\gamma_{y}(1)}{\\gamma_{y}(0)}=\\frac{cov(y_\\{t+1}, y_t)}{\\sigma_{y}^2}$$\n",
    "\n",
    "The expected value of \\\\(y_{t}\\\\) is \\\\(E[y_{t}]=\\mu_y=5\\\\) and the variance is \\\\(var(y_{t})=\\sigma_{x}^2(1+0.7^2)\\\\) due to linearity. \n",
    "\n",
    "$$cov(y_t+1, y_t) = E[(5+x_\\{t+1}-0.7x_\\{t-1+1}-\\mu_y)(5+x_\\{t}-0.7x_\\{t-1}-\\mu_y)]= E[(-0.7x_{t}x_{t}+0.7^2x_{t}^2)]=-0.7E[x_{t}^2]=-0.7\\sigma_{x}^2$$\n",
    "\n",
    "$$\\rho_{y}(1)=\\frac{-0.7\\sigma_{x}^2}{\\sigma_{x}^2(1+0.7^2)}=\\frac{-0.7}{1+0.7^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf7b7ff-0f95-4941-b2bf-964d0c0cb3fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## To convince ourselves that this is indeed correct, the series is simulated and the sample autocorrelation is found\n",
    "num_samples = [100*i for i in range(100)]\n",
    "lag = 1\n",
    "acf_lag_empiricals = []\n",
    "\n",
    "for num in num_samples:\n",
    "    mu_x, sigma_x = 0, 1\n",
    "    x = np.random.normal(mu_x, sigma_x, num)\n",
    "    y = np.zeros(num)\n",
    "\n",
    "    for i in range(num):\n",
    "        x_last = 0\n",
    "        if i-1 >= 0:\n",
    "            x_last = x[i-1] \n",
    "        y[i] = 5 + x[i] -0.7*x_last\n",
    "\n",
    "    acf_lag_empiricals += [autocorrelation(y, lag=lag)]\n",
    "\n",
    "acf_lag_theroretical = -0.7/(1+0.7**2)\n",
    "\n",
    "print(f\"ACF Lag={lag}, N={num_samples[-1]}. Empirical={acf_lag_empiricals[-1]:.3f}, Theoretical={acf_lag_theroretical:.3f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(num_samples, acf_lag_empiricals, label=\"Empirical\")\n",
    "plt.plot(num_samples, [acf_lag_theroretical for num in num_samples], label=\"Theoretical\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(f\"Empirical vs theoretical value for ACF Lag={lag}\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Correlation and Covariance of stochastic processes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
